seed: 42
work_dir: "."
model: configs/models/model.yaml
method: configs/methods/sft_lora.yaml

templating:
  mode: base  # base | chat
  prompts:
    base_instruction: |
      You are a deterministic converter to UNIMARC in XML.
      Rules :
      - ONLY returns a valid and well-formed XML.
      - UTF-8 encoding, no comment, no text out of tags.
      - Use the relevant Unimarc fields (200, 210, 700â€¦).
      - If an info is missing, omit the corresponding field (does not invent anything).

      Metadata :
      {metadata}

      XML UNIMARC :
    system_prompt: |
      You are a deterministic converter to UNIMARC in XML. Output only valid XML, well-formed and UTF-8 encoded.

train:
  max_steps: 300
  num_train_epochs: 1
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  lr: 2e-4
  warmup_ratio: 0.03
  logging_steps: 25
  eval_steps: 100
  save_steps: 100
  bf16: true
  fp16: false
  gradient_checkpointing: true
  num_proc: 2
  max_length: 1024

paths:
  train: data/processed/train.jsonl
  eval: data/eval/heldout.jsonl
  out: runs/

data:
  source: "hf"
  repo: "Geraldine/metadata-to-unimarc-reasoning"
  prompt_cols: ["metadata"]
  label_col: "unimarc_record"

hf_job:
  instance_type: "gpu_1x_a10g"
  # Hugging Face Jobs parameters for the sft script
  script_args:
    dataset_name: "Geraldine/Unimarc-iln050-5k"
    dataset_train_split: "train"
    dataset_test_split: "test"
  model_args:
    model_name_or_path: "unsloth/LFM2-350M"
    use_peft: true
    lora_r: 16
    lora_alpha: 16
  training_args:
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    warmup_steps: 5
    max_steps: 100
    learning_rate: 0.00005
    logging_steps: 1
    output_dir: "/tmp/output" # Relative to the job container
    optim: "adamw_8bit"
    do_eval: false
